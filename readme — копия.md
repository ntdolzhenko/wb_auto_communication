# RAG-модель поддержки на Flask

Этот проект представляет собой систему автоматического ответа на вопросы пользователей, основанную на архитектуре **RAG (Retrieval-Augmented Generation)**. Модель генерирует ответы с учётом контекста из:

- ранее заданных вопросов и ответов (QA),
- базы знаний (KB).

Все модели работают локально через [Ollama](https://ollama.com/), включая эмбеддер (DeepPavlov) и генеративную LLM-модель (geemaa3).

---

## Стек технологий

- **LLM:** geemaa3 (через Ollama)
- **Ретривер:** FAISS + TF-IDF + Deeppavlov embedding
- **Фреймворк:** Flask
- **Формирование контекста:** гибридный поиск (QA + KB)

---

## Структура проекта

```
.
├── app/
│   ├── model.py
│   ├── routes.py
├── data/
│   ├── файлы данных, эмбеддингов, индексов и т.д.
├── templates/
│   └── index.html
├── run_app.py
├── environment.yml
└── README.md
```

---

## Инструкции по установке

1. **Создайте виртуальное окружение**

   ```bash
   conda env create -f environment.yml --name flask-rag
   ```

   > При желании вместо flask-rag можно задать другое имя окружения.

2. **Активируйте окружение**

   ```bash
   conda activate flask-rag
   ```

3. **Запустите сервер**

   ```bash
   python run_app.py
   ```

4. **Откройте локальный веб-интерфейс**

   [http://127.0.0.1:5000](http://127.0.0.1:5000)

---

## Как это работает

1. Пользователь задаёт вопрос через форму.
2. Вопрос обрабатывается эмбеддинг-моделью.
3. Происходит поиск среди:
   - похожих вопросов,
   - базы знаний.
4. Из них собирается контекст.
5. Он передаётся в LLM.
6. Система возвращает готовый ответ.

---

## Заметки
- Для загрузки сохранённых `.pkl` и `.faiss` файлов рекомендуется соблюдать версии:
  - `scikit-learn==1.6.1`
  - `numpy==1.26.4`
  - `faiss-cpu==1.9.0`

